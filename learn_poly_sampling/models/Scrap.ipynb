{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93da5142",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\WPy64-38100\\python-3.8.10.amd64\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:513: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  np.object,\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Optimizer, Adam\n",
    "from typing import Optional,Callable,Any\n",
    "from torchvision.models.resnet import ResNet,_resnet,conv1x1,conv3x3\n",
    "from torchvision.models import WeightsEnum\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics.functional.classification.accuracy import accuracy\n",
    "import pytorch_lightning as pl\n",
    "import numpy as np\n",
    "import logging\n",
    "import sys\n",
    "sys.path.append(\"C:\\Files\\Github Repo\\TrainLPS\\learn_poly_sampling\")\n",
    "from layers import get_pool_method\n",
    "from layers.polydown import set_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a54c1689",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbstractBaseClassifierModel(pl.LightningModule):\n",
    "    \"\"\"Abstract class classifiers.\n",
    "    Reusable code for clasifier models.\n",
    "    To make new classifiers, just implement the initializer and the forward method\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 optimizer=None, optimizer_kwargs={},\n",
    "                 scheduler=None, scheduler_kwargs={},\n",
    "                 param_scheduler=None, param_scheduler_kwargs={},\n",
    "                 warmup_epochs=0, eval_mode='class_accuracy',\n",
    "                 shift_seed=7,shift_max=None,\n",
    "                 shift_samples=None,shift_patch_size=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.optimizer_fn = optimizer if optimizer is not None else torch.optim.Adam\n",
    "        self.optimizer_kwargs = optimizer_kwargs\n",
    "        self.scheduler_fn = scheduler\n",
    "        self.scheduler_kwargs = scheduler_kwargs\n",
    "        self.param_scheduler_fn = param_scheduler\n",
    "        self.param_scheduler_kwargs = param_scheduler_kwargs\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "\n",
    "        # Evaluation settings\n",
    "        self.eval_mode = eval_mode\n",
    "        self.shift_seed = shift_seed\n",
    "        self.shift_max = shift_max\n",
    "        self.shift_samples = shift_samples\n",
    "        self.shift_patch_size = shift_patch_size\n",
    "        logging.info(f'Evaluation mode: {self.eval_mode}')\n",
    "\n",
    "    # logic for a single training step\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "\n",
    "        # training metrics\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = accuracy(preds, y)\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, logger=True)\n",
    "        self.log('train_acc', acc, on_step=True, on_epoch=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    # logic for a single validation step\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "\n",
    "        # validation metrics\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = accuracy(preds, y)\n",
    "        self.log('val_loss',loss,prog_bar=True,sync_dist=True)\n",
    "        self.log('val_acc',acc,prog_bar=True,sync_dist=True)\n",
    "        return loss\n",
    "\n",
    "    def on_test_start(self) -> None:\n",
    "        \"\"\"Called when the test begins.\"\"\"\n",
    "        np.random.seed(self.shift_seed)\n",
    "\n",
    "    # logic for a single testing step\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "\n",
    "        # test metrics\n",
    "        if self.eval_mode=='class_accuracy':\n",
    "            preds = torch.argmax(logits,dim=1)\n",
    "            acc = accuracy(preds,y)\n",
    "            self.log('test_loss',loss,prog_bar=True,sync_dist=True)\n",
    "            self.log('test_acc',acc,prog_bar=True,sync_dist=True)\n",
    "        elif self.eval_mode=='shift_consistency':\n",
    "            self.eval_consistency_step(batch,batch_idx,'test','shift')\n",
    "            self.eval_consistency_step(batch,batch_idx,'test','circular')\n",
    "        return loss\n",
    "\n",
    "    def eval_consistency_step(self, batch, batch_idx,\n",
    "                              val_mode='test', mode='shift'):\n",
    "        assert mode in ['shift', 'circular']\n",
    "        x,_ = batch\n",
    "\n",
    "        # Read dataset from 'inc_conv1_support'\n",
    "        if self.inc_conv1_support: dataset = 'imagenet'\n",
    "        else: dataset = 'cifar10'\n",
    "\n",
    "        if dataset=='imagenet':\n",
    "            outputs = []\n",
    "            if mode == 'shift':\n",
    "                # Pass shifted inputs\n",
    "                offsets = [np.random.randint(self.shift_max,size=2) for j in range(0,self.shift_samples)]\n",
    "                for j in range(0,self.shift_samples):\n",
    "                    outputs.append(self(x[:,:,offsets[j][0]:offsets[j][0]+self.shift_patch_size,offsets[j][1]:offsets[j][1]+self.shift_patch_size]))\n",
    "                # Compute consistency\n",
    "                cur_agree = self.agreement(outputs,self.shift_samples).type(torch.FloatTensor).to(outputs[0].device)\n",
    "            elif mode == 'circular':\n",
    "                # Pass rolled inputs\n",
    "                # -max to max for comparison purposes\n",
    "                offsets = [np.random.randint(-self.shift_max,self.shift_max,size=2) for j in range(0,self.shift_samples)]\n",
    "                for j in range(0,self.shift_samples):\n",
    "                    outputs.append(self(torch.roll(x,shifts=(offsets[j][0],offsets[j][1]),dims=(2,3))))\n",
    "                # Compute consistency\n",
    "                cur_agree = self.agreement(outputs,self.shift_samples).type(torch.FloatTensor).to(outputs[0].device)\n",
    "        elif dataset=='cifar10':\n",
    "            max_shift = 3\n",
    "            random_shift1 = torch.randint(-max_shift, max_shift, (2,))\n",
    "            random_shift2 = torch.randint(-max_shift, max_shift, (2,))\n",
    "            if mode == 'shift':\n",
    "                pad_lengths = (max_shift, max_shift, max_shift, max_shift)\n",
    "                i1_l, i1_r = random_shift1[0] + max_shift, random_shift1[0]-max_shift\n",
    "                j1_l, j1_r = random_shift1[1] + max_shift, random_shift1[1]-max_shift\n",
    "                i2_l, i2_r = random_shift2[0] + max_shift, random_shift2[0]-max_shift\n",
    "                j2_l, j2_r = random_shift2[1] + max_shift, random_shift2[1]-max_shift\n",
    "                shifted_x1 = F.pad(x, pad_lengths)[:, :, i1_l:i1_r, j1_l:j1_r ]\n",
    "                shifted_x2 = F.pad(x, pad_lengths)[:, :, i2_l:i2_r, j2_l:j2_r ]\n",
    "            elif mode == 'circular':\n",
    "                shifted_x1 = torch.roll(x, shifts = (random_shift1[0], random_shift1[1]), dims = (2, 3))\n",
    "                shifted_x2 = torch.roll(x, shifts = (random_shift2[0], random_shift2[1]), dims = (2, 3))\n",
    "\n",
    "            # Compute consistency\n",
    "            shifted_preds1 = torch.argmax(self(shifted_x1),1)\n",
    "            shifted_preds2 = torch.argmax(self(shifted_x2),1)\n",
    "            cur_agree = accuracy(shifted_preds1,shifted_preds2)\n",
    "        self.log('%s_%s_consistency' % (val_mode, mode), cur_agree, prog_bar=True)\n",
    "        return\n",
    "\n",
    "    # (Core) Compute consistency\n",
    "    def agreement(self,outputs,robust_num):\n",
    "        preds = torch.stack([output.argmax(dim=1,keepdim=False) for output in outputs], dim=0)\n",
    "        similarity = torch.sum((preds == preds[0:1,:]).int(), dim=0)\n",
    "        agree = 100*torch.mean((similarity == robust_num).float())\n",
    "        return agree\n",
    "\n",
    "    def optimizer_step( self,epoch,batch_idx,\n",
    "                        optimizer,optimizer_idx,optimizer_closure,\n",
    "                        on_tpu,using_native_amp,using_lbfgs):\n",
    "        \"\"\"Called when the train epoch begins.\"\"\"\n",
    "        # Lr warmup\n",
    "        if self.current_epoch < self.warmup_epochs:\n",
    "            it_curr = self.trainer.num_training_batches*self.current_epoch+1+batch_idx\n",
    "            it_max = self.trainer.num_training_batches*self.warmup_epochs\n",
    "            lr_scale = float(it_curr) / it_max\n",
    "            for pg in self.trainer.optimizers[0].param_groups:\n",
    "                pg['lr'] = lr_scale * self.learning_rate\n",
    "\n",
    "        # Update params\n",
    "        optimizer.step(closure=optimizer_closure)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        logging.info(f'Configuring optimizer: {self.optimizer_fn} with {self.optimizer_kwargs}')\n",
    "\n",
    "        # Filter net/pool parameters\n",
    "        net_pars, pool_pars = [], []\n",
    "        for n,p in self.named_parameters():\n",
    "            if \"component_selection\" in n and p.requires_grad_:\n",
    "                pool_pars.append(p)\n",
    "            elif p.requires_grad_:\n",
    "                net_pars.append(p)\n",
    "\n",
    "        # Set opt\n",
    "        # Start lr at 0 if warmup #moved to WarmUpScheduler\n",
    "        #_lr = self.learning_rate\n",
    "        #print('*************************************************************_lr',_lr)\n",
    "        _lr= 0 if self.warmup_epochs!=0 else self.learning_rate\n",
    "        #optimizer = self.optimizer_fn([{'params': net_pars,\n",
    "        #                                'lr': _lr},\n",
    "        #                               {'params': pool_pars,\n",
    "        #                                'weight_decay': 0,\n",
    "        #                                'lr': _lr}],\n",
    "        #                              lr=_lr,\n",
    "        #                              **self.optimizer_kwargs)\n",
    "        optimizer = self.optimizer_fn([{'params': net_pars},\n",
    "                                       {'params': pool_pars,'weight_decay': 0}],\n",
    "                                      lr=_lr,\n",
    "                                      **self.optimizer_kwargs)\n",
    "\n",
    "        if self.scheduler_fn is None:\n",
    "            return optimizer\n",
    "\n",
    "        logging.info(f'Configuring lr scheduler: {self.scheduler_fn} with {self.scheduler_kwargs}')\n",
    "        if isinstance(self.scheduler_fn, list):\n",
    "            schedulers = [sch_fn(optimizer, **sch_kwargs) for sch_fn, sch_kwargs in zip(self.scheduler_fn, self.scheduler_kwargs)]\n",
    "        else:\n",
    "            schedulers = [\n",
    "                {'scheduler': self.scheduler_fn(optimizer, **self.scheduler_kwargs),\n",
    "                 'frequency': 1,\n",
    "                 'name': 'main_lr_scheduler'\n",
    "                 }]\n",
    "\n",
    "        #if self.warmup_epochs > 0:\n",
    "        #    schedulers.append({'scheduler': WarmupScheduler(optimizer,\n",
    "        #                                                    warmup_steps=self.warmup_epochs),\n",
    "        #                       'frequency': 1,\n",
    "        #                       'name': 'warmup_scaled_lr'})\n",
    "\n",
    "        return [optimizer], schedulers\n",
    "\n",
    "    def configure_callbacks(self):\n",
    "        if self.param_scheduler_fn is not None:\n",
    "            logging.info(f'Configuring tau scheduler: {self.param_scheduler_fn} with {self.param_scheduler_kwargs}')\n",
    "            return [self.param_scheduler_fn('gumbel_tau', **self.param_scheduler_kwargs)]\n",
    "        return super().configure_callbacks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ff1077",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b94b5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_pool_method(name, FLAGS):\n",
    "#     #different pool methods uses different flags, this needs cleaning up\n",
    "#     _available_pool_methods = ('max_2_norm', 'LPS', 'avgpool', 'Decimation', 'skip')\n",
    "#     assert name in _available_pool_methods\n",
    "#     antialias_layer = get_antialias(antialias_mode=FLAGS.antialias_mode,\n",
    "#                                     antialias_size=FLAGS.antialias_size,\n",
    "#                                     antialias_padding=FLAGS.antialias_padding,\n",
    "#                                     antialias_padding_mode=FLAGS.antialias_padding_mode,\n",
    "#                                     antialias_group=FLAGS.antialias_group)\n",
    "#     pool_method = {\n",
    "#         'max_2_norm': partial(\n",
    "#             PolyphaseInvariantDown2D,\n",
    "#             component_selection=max_p_norm,\n",
    "#             antialias_layer=antialias_layer,\n",
    "#             selection_noantialias=FLAGS.selection_noantialias,\n",
    "#         ),\n",
    "#         'LPS': partial(\n",
    "#             PolyphaseInvariantDown2D,\n",
    "#             component_selection= LPS,\n",
    "#             get_logits=get_logits_model(FLAGS.logits_model),\n",
    "#             logits_pad=FLAGS.LPS_pad,\n",
    "#             comp_fix_train=FLAGS.LPS_gumbel,\n",
    "#             comp_train_convex=FLAGS.LPS_train_convex,\n",
    "#             comp_convex=FLAGS.LPS_convex,\n",
    "#             antialias_layer=antialias_layer,\n",
    "#             selection_noantialias=FLAGS.selection_noantialias,\n",
    "#         ),\n",
    "#         'Decimation': partial(\n",
    "#             Decimation,\n",
    "#             stride=FLAGS.pool_k,\n",
    "#             antialias_layer=antialias_layer,\n",
    "#         ),\n",
    "#         'avgpool': partial(\n",
    "#             nn.AvgPool2d,\n",
    "#             kernel_size=FLAGS.pool_k\n",
    "#         ),\n",
    "#         'skip': None\n",
    "#     }\n",
    "#     return pool_method[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9238628c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Circular padding (class)\n",
    "class cpad(nn.Module):\n",
    "  def __init__(self, pad):\n",
    "    super(cpad,self).__init__()\n",
    "    self.pad = pad\n",
    "  def forward(self,x):\n",
    "    return F.pad(x, pad = self.pad, mode = 'circular')\n",
    "  def extra_repr(self):\n",
    "    return (\"pad={pad}\".format(pad = self.pad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4134740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace and initialize conv\n",
    "def replace_conv(in_ch,out_ch,kernel_size,\n",
    "                 padding,padding_mode,init,\n",
    "                 bias=False,stride=1):\n",
    "  c=nn.Conv2d(in_channels=in_ch,\n",
    "              out_channels=out_ch,\n",
    "              kernel_size=kernel_size,\n",
    "              padding=padding,\n",
    "              padding_mode=padding_mode,\n",
    "              bias=bias,\n",
    "              stride=stride)\n",
    "  if init:\n",
    "    nn.init.kaiming_normal_(c.weight,\n",
    "                            mode='fan_out',\n",
    "                            nonlinearity='relu')\n",
    "  return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f91fbb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace and initialize pool\n",
    "def replace_pool(p,in_ch,out_ch,\n",
    "                 kernel_size,padding,padding_mode,\n",
    "                 init,bn,swap_conv_pool=False):\n",
    "  # Conv\n",
    "  c=nn.Conv2d(in_ch,\n",
    "              out_ch,\n",
    "              kernel_size=kernel_size,\n",
    "              padding=padding,\n",
    "              padding_mode=padding_mode,\n",
    "              bias=False)\n",
    "  if init:\n",
    "    # Kaiming init.\n",
    "    nn.init.kaiming_normal_(c.weight,\n",
    "                            mode='fan_out',\n",
    "                            nonlinearity='relu')\n",
    "\n",
    "  if bn:\n",
    "    # Include BN\n",
    "    b=nn.BatchNorm2d(out_ch)\n",
    "    if init:\n",
    "      # Constant init.\n",
    "      nn.init.constant_(b.weight,1)\n",
    "      nn.init.constant_(b.bias,0)\n",
    "    if swap_conv_pool: s=nn.Sequential(c,b,p) # Pool applied last\n",
    "    else: s=nn.Sequential(p,c,b)\n",
    "  else:\n",
    "    if swap_conv_pool: s=nn.Sequential(c,p)\n",
    "    else: s=nn.Sequential(p,c)\n",
    "  return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "22380586",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BottleneckLiteCustom(nn.Module):\n",
    "    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n",
    "    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n",
    "    # according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385.\n",
    "    # This variant is also known as ResNet V1.5 and improves accuracy according to\n",
    "    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n",
    "\n",
    "    expansion: int = 2\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        inplanes: int,\n",
    "        planes: int,\n",
    "        stride: int = 1,\n",
    "        downsample: Optional[nn.Module] = None,\n",
    "        groups: int = 1,\n",
    "        base_width: int = 64,\n",
    "        dilation: int = 1,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "    ) -> None:\n",
    "        super(BottleneckLiteCustom, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        width = int(planes * (base_width / 64.0 / 2.0)) * groups\n",
    "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv1x1(inplanes, width)\n",
    "        self.bn1 = norm_layer(width)\n",
    "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
    "        self.bn2 = norm_layer(width)\n",
    "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
    "        self.bn3 = norm_layer(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x: Tensor, global_ret_prob=False) -> Tensor:\n",
    "        # TODO: Generalize to any sequential length\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        if self.stride>1 and self.ret_prob:\n",
    "          # Return feats and prob\n",
    "          if self.swap_conv_pool:\n",
    "            # conv->pool\n",
    "            out=self.conv3[0](out)\n",
    "            out,_p=self.conv3[1](x=out,ret_prob=True)\n",
    "          else:\n",
    "            # pool->conv\n",
    "            out,_p=self.conv3[0](x=out,ret_prob=True)\n",
    "            out=self.conv3[1](out)\n",
    "          out=self.bn3(out)\n",
    "          if self.downsample is not None:\n",
    "            # Pass original input and prob to downsample\n",
    "            if self.forward_pool_method==\"LPS\" and self.training:\n",
    "              # Train: If LPS, prob is first element of tuple\n",
    "              p = _p[0] \n",
    "            else:\n",
    "              p = _p\n",
    "            identity=self.downsample[0](x=x,prob=p)\n",
    "            identity=self.downsample[1](identity)\n",
    "            identity=self.downsample[2](identity)\n",
    "        else:\n",
    "          # Original pipeline\n",
    "          out = self.conv3(out)\n",
    "          out = self.bn3(out)\n",
    "          if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        if global_ret_prob:\n",
    "          # Train: Return feats and probability-logits tuple\n",
    "          # Test: return feats and logits\n",
    "          return out,_p\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3c0196d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core ResNet50, fixed shortcut via BottleneckCustom\n",
    "def resnet50_fs(pretrained: WeightsEnum = None, progress: bool = True, **kwargs: Any) -> ResNet:\n",
    "    return _resnet(BottleneckLiteCustom, [3, 4, 6, 3], pretrained, progress,\n",
    "                   **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e86af340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom ResNet50 (ImageNet)\n",
    "class ResNet50LiteCustom(AbstractBaseClassifierModel):\n",
    "  def __init__(self,input_shape,num_classes,\n",
    "               padding_mode='zeros',learning_rate=0.1,pooling_layer=nn.AvgPool2d,\n",
    "               extras_model=None,**kwargs):\n",
    "    super().__init__(**kwargs)\n",
    " \n",
    "    # log hyperparameters\n",
    "    self.save_hyperparameters()\n",
    "    self.learning_rate=learning_rate\n",
    "\n",
    "    # Model-specific extras\n",
    "    self.logits_channels=extras_model['logits_channels']\n",
    "    self.conv1_stride=extras_model['conv1_stride']\n",
    "    self.maxpool_zpad=extras_model['maxpool_zpad']\n",
    "    self.swap_conv_pool=extras_model['swap_conv_pool']\n",
    "    self.inc_conv1_support=extras_model['inc_conv1_support']\n",
    "    self.apply_maxpool=extras_model['apply_maxpool']\n",
    "    self.ret_prob=extras_model['ret_prob']\n",
    "    self.forward_pool_method = extras_model['forward_pool_method'] if 'forward_pool_method' in extras_model.keys()\\\n",
    "      else 'LPS'\n",
    "\n",
    "    # ResNet50 model with fixed shortcut\n",
    "    self.core=resnet50_fs()\n",
    "\n",
    "    # Modify Conv2d padding attribute\n",
    "    for layer in self.core.modules():\n",
    "      if isinstance(layer,nn.Conv2d):\n",
    "        layer.padding_mode=padding_mode\n",
    "\n",
    "    # Pass extras to BasicBlock\n",
    "    for i in range(len(self.core.layer1)):\n",
    "      self.core.layer1[i].ret_prob= self.ret_prob\n",
    "      self.core.layer1[i].swap_conv_pool= self.swap_conv_pool\n",
    "      self.core.layer1[i].forward_pool_method = self.forward_pool_method\n",
    "    for i in range(len(self.core.layer2)):\n",
    "      self.core.layer2[i].ret_prob= self.ret_prob\n",
    "      self.core.layer2[i].swap_conv_pool= self.swap_conv_pool\n",
    "      self.core.layer2[i].forward_pool_method = self.forward_pool_method\n",
    "    for i in range(len(self.core.layer3)):\n",
    "      self.core.layer3[i].ret_prob= self.ret_prob\n",
    "      self.core.layer3[i].swap_conv_pool= self.swap_conv_pool\n",
    "      self.core.layer3[i].forward_pool_method = self.forward_pool_method\n",
    "    for i in range(len(self.core.layer4)):\n",
    "      self.core.layer4[i].ret_prob= self.ret_prob\n",
    "      self.core.layer4[i].swap_conv_pool= self.swap_conv_pool\n",
    "      self.core.layer4[i].forward_pool_method = self.forward_pool_method\n",
    "\n",
    "    if pooling_layer is None:\n",
    "      # Keep original pool\n",
    "      pass\n",
    "    else:\n",
    "      # Replace pool\n",
    "      # Logits model channels\n",
    "      if self.logits_channels:\n",
    "        maxpool_h_ch = self.logits_channels[\"maxpool\"]\n",
    "        layer2_h_ch = self.logits_channels[\"layer2\"]\n",
    "        layer3_h_ch = self.logits_channels[\"layer3\"]\n",
    "        layer4_h_ch = self.logits_channels[\"layer4\"]\n",
    "      else:\n",
    "        maxpool_h_ch = 64\n",
    "        layer2_h_ch = 128\n",
    "        layer3_h_ch = 256\n",
    "        layer4_h_ch = 512\n",
    "\n",
    "      if self.inc_conv1_support:\n",
    "        # ImageNet/Imagenette: Update conv1 stride\n",
    "        conv1_stride=2 if self.conv1_stride else 1\n",
    "        self.core.conv1=replace_conv(in_ch=3,\n",
    "                                     out_ch=64,\n",
    "                                     kernel_size=7,\n",
    "                                     padding=3,\n",
    "                                     padding_mode=padding_mode,\n",
    "                                     stride=conv1_stride,\n",
    "                                     init=True)\n",
    "\n",
    "      if self.apply_maxpool:\n",
    "        # ImageNet/Imagenette: Replace maxpool stride by custom pool\n",
    "        _maxpool = []\n",
    "        if self.conv1_stride:\n",
    "          # Conv1 stride applied already\n",
    "          pass\n",
    "        else:\n",
    "          # Replace conv1 stride by custom pool\n",
    "          _maxpool.append(set_pool(pooling_layer=pooling_layer,\n",
    "                                   p_ch=64,\n",
    "                                   h_ch=maxpool_h_ch,\n",
    "                                   no_antialias=True))\n",
    "        if self.maxpool_zpad:\n",
    "          _maxpool.append(nn.ZeroPad2d((0,1,0,1)))\n",
    "        else:\n",
    "          _maxpool.append(cpad(pad=[0,1,0,1]))\n",
    "        _maxpool.append(nn.MaxPool2d(kernel_size=2,\n",
    "                        stride=1))\n",
    "        _maxpool.append(set_pool(pooling_layer=pooling_layer,\n",
    "                        p_ch=64,\n",
    "                        h_ch=maxpool_h_ch))\n",
    "        self.core.maxpool=nn.Sequential(*_maxpool)\n",
    "\n",
    "      # Replace stride [layer2, layer3, layer4]\n",
    "      # Set main branch pool\n",
    "      p2_1=set_pool(pooling_layer=pooling_layer,\n",
    "                    p_ch=128,\n",
    "                    h_ch=layer2_h_ch)\n",
    "      p3_1=set_pool(pooling_layer=pooling_layer,\n",
    "                    p_ch=256,\n",
    "                    h_ch=layer3_h_ch)\n",
    "      p4_1=set_pool(pooling_layer=pooling_layer,\n",
    "                    p_ch=512,\n",
    "                    h_ch=layer4_h_ch)\n",
    "\n",
    "      # Replace and init. layers\n",
    "      self.core.layer2[0].conv2=replace_conv(in_ch=64,\n",
    "                                             out_ch=64,\n",
    "                                             kernel_size=3,\n",
    "                                             padding=1,\n",
    "                                             padding_mode=padding_mode,\n",
    "                                             init=True)\n",
    "      self.core.layer2[0].conv3=replace_pool(p=p2_1,\n",
    "                                             in_ch=128,\n",
    "                                             out_ch=256,\n",
    "                                             kernel_size=1,\n",
    "                                             padding=0,\n",
    "                                             padding_mode=padding_mode,\n",
    "                                             swap_conv_pool= self.swap_conv_pool,\n",
    "                                             init=True,\n",
    "                                             bn=False)\n",
    "      self.core.layer3[0].conv2=replace_conv(in_ch=128,\n",
    "                                             out_ch=128,\n",
    "                                             kernel_size=3,\n",
    "                                             padding=1,\n",
    "                                             padding_mode=padding_mode,\n",
    "                                             init=True)\n",
    "      self.core.layer3[0].conv3=replace_pool(p=p3_1,\n",
    "                                             in_ch=256,\n",
    "                                             out_ch=512,\n",
    "                                             kernel_size=1,\n",
    "                                             padding=0,\n",
    "                                             padding_mode=padding_mode,\n",
    "                                             swap_conv_pool= self.swap_conv_pool,\n",
    "                                             init=True,\n",
    "                                             bn=False)\n",
    "      self.core.layer4[0].conv2=replace_conv(in_ch=256,\n",
    "                                             out_ch=256,\n",
    "                                             kernel_size=3,\n",
    "                                             padding=1,\n",
    "                                             padding_mode=padding_mode,\n",
    "                                             init=True)\n",
    "      self.core.layer4[0].conv3=replace_pool(p=p4_1,\n",
    "                                             in_ch=512,\n",
    "                                             out_ch=1024,\n",
    "                                             kernel_size=1,\n",
    "                                             padding=0,\n",
    "                                             padding_mode=padding_mode,\n",
    "                                             swap_conv_pool= self.swap_conv_pool,\n",
    "                                             init=True,\n",
    "                                             bn=False)\n",
    "\n",
    "      # Set shortcut branch pool\n",
    "      # No component selection, indices precomputed\n",
    "      # https://github.com/pytorch/vision/blob/863e904e4165fe42950c355325a93198d56e4271/torchvision/models/resnet.py#L78\n",
    "      p2_2=set_pool(pooling_layer=pooling_layer,\n",
    "                    p_ch=256,\n",
    "                    use_get_logits=False)\n",
    "      p3_2=set_pool(pooling_layer=pooling_layer,\n",
    "                    p_ch=512,\n",
    "                    use_get_logits=False)\n",
    "      p4_2=set_pool(pooling_layer=pooling_layer,\n",
    "                    p_ch=1024,\n",
    "                    use_get_logits=False)\n",
    "\n",
    "      # Replace and init. layers\n",
    "      # Ksize=1, no padding required\n",
    "      self.core.layer2[0].downsample=replace_pool(p=p2_2,\n",
    "                                                  in_ch=256,\n",
    "                                                  out_ch=256,\n",
    "                                                  kernel_size=1,\n",
    "                                                  padding=0,\n",
    "                                                  padding_mode=padding_mode,\n",
    "                                                  init=True,\n",
    "                                                  bn=True)\n",
    "      self.core.layer3[0].downsample=replace_pool(p=p3_2,\n",
    "                                                  in_ch=512,\n",
    "                                                  out_ch=512,\n",
    "                                                  kernel_size=1,\n",
    "                                                  padding=0,\n",
    "                                                  padding_mode=padding_mode,\n",
    "                                                  init=True,\n",
    "                                                  bn=True)\n",
    "      self.core.layer4[0].downsample=replace_pool(p=p4_2,\n",
    "                                                  in_ch=1024,\n",
    "                                                  out_ch=1024,\n",
    "                                                  kernel_size=1,\n",
    "                                                  padding=0,\n",
    "                                                  padding_mode=padding_mode,\n",
    "                                                  init=True,\n",
    "                                                  bn=True)\n",
    "\n",
    "    # Replace head\n",
    "    self.core.fc= nn.Linear(1024,num_classes)\n",
    "\n",
    "  def forward(self,x):\n",
    "    out=self.core(x)\n",
    "    out=F.log_softmax(out,dim=1)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3a28bf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "extras_model = {\n",
    "    'maxpool_zpad': True,\n",
    "    'swap_conv_pool': False,\n",
    "    'inc_conv1_support': True,\n",
    "    'apply_maxpool': True,\n",
    "    'ret_prob': True,\n",
    "    'logits_channels': None,\n",
    "    'conv1_stride': False,\n",
    "}\n",
    "\n",
    "class flag_struct():\n",
    "  def __init__(self):\n",
    "    self.dryrun = False\n",
    "    self.pool_method = 'LPS'\n",
    "    self.logits_model = 'LPSLogitLayers'\n",
    "    self.swap_conv_pool = False\n",
    "    self.maxpool_zpad = False\n",
    "    self.LPS_pad = 'circular'\n",
    "    self.LPS_debug = False\n",
    "    self.LPS_convex = False\n",
    "    self.pool_k = 2\n",
    "    self.antialias_mode = 'LowPassFilter'\n",
    "    self.antialias_size = 3\n",
    "    self.antialias_padding = 'same'\n",
    "    self.antialias_padding_mode = \"circular\"\n",
    "    self.antialias_group = 8\n",
    "    self.selection_noantialias = False\n",
    "    self.LPS_gumbel = False\n",
    "    self.LPS_train_convex = False\n",
    "\n",
    "\n",
    "FLAGS = flag_struct()\n",
    "pool_layer = get_pool_method(FLAGS.pool_method, FLAGS)\n",
    "model = ResNet50LiteCustom(224, 1000, extras_model = extras_model, pooling_layer=pool_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "58a738bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-7.2343, -7.2820, -6.7135,  ..., -7.2695, -7.5290, -7.1022],\n",
       "        [-7.0196, -7.3989, -7.0052,  ..., -7.6473, -6.9428, -6.9855]],\n",
       "       grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.rand(2,3,224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1de3af10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14302376\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "58156a42",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet50LiteCustom(\n",
       "  (core): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): Sequential(\n",
       "      (0): PolyphaseInvariantDown2D(\n",
       "        (component_selection): LPS(\n",
       "          (get_logits): LPSLogitLayersV2(\n",
       "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same, padding_mode=circular)\n",
       "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same, padding_mode=circular)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): ZeroPad2d((0, 1, 0, 1))\n",
       "      (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "      (3): PolyphaseInvariantDown2D(\n",
       "        (component_selection): LPS(\n",
       "          (get_logits): LPSLogitLayersV2(\n",
       "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same, padding_mode=circular)\n",
       "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same, padding_mode=circular)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (_antialias_layer): LowPassFilter(in_channels=64, filter_size=3, padding=same, padding_mode=circular)\n",
       "      )\n",
       "    )\n",
       "    (layer1): Sequential(\n",
       "      (0): BottleneckLiteCustom(\n",
       "        (conv1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BottleneckLiteCustom(\n",
       "        (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): BottleneckLiteCustom(\n",
       "        (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BottleneckLiteCustom(\n",
       "        (conv1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Sequential(\n",
       "          (0): PolyphaseInvariantDown2D(\n",
       "            (component_selection): LPS(\n",
       "              (get_logits): LPSLogitLayersV2(\n",
       "                (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same, padding_mode=circular)\n",
       "                (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same, padding_mode=circular)\n",
       "                (relu): ReLU()\n",
       "              )\n",
       "            )\n",
       "            (_antialias_layer): LowPassFilter(in_channels=128, filter_size=3, padding=same, padding_mode=circular)\n",
       "          )\n",
       "          (1): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): PolyphaseInvariantDown2D(\n",
       "            (component_selection): LPS()\n",
       "            (_antialias_layer): LowPassFilter(in_channels=256, filter_size=3, padding=same, padding_mode=circular)\n",
       "          )\n",
       "          (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BottleneckLiteCustom(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): BottleneckLiteCustom(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): BottleneckLiteCustom(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BottleneckLiteCustom(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Sequential(\n",
       "          (0): PolyphaseInvariantDown2D(\n",
       "            (component_selection): LPS(\n",
       "              (get_logits): LPSLogitLayersV2(\n",
       "                (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same, padding_mode=circular)\n",
       "                (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same, padding_mode=circular)\n",
       "                (relu): ReLU()\n",
       "              )\n",
       "            )\n",
       "            (_antialias_layer): LowPassFilter(in_channels=256, filter_size=3, padding=same, padding_mode=circular)\n",
       "          )\n",
       "          (1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): PolyphaseInvariantDown2D(\n",
       "            (component_selection): LPS()\n",
       "            (_antialias_layer): LowPassFilter(in_channels=512, filter_size=3, padding=same, padding_mode=circular)\n",
       "          )\n",
       "          (1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BottleneckLiteCustom(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): BottleneckLiteCustom(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): BottleneckLiteCustom(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): BottleneckLiteCustom(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): BottleneckLiteCustom(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BottleneckLiteCustom(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Sequential(\n",
       "          (0): PolyphaseInvariantDown2D(\n",
       "            (component_selection): LPS(\n",
       "              (get_logits): LPSLogitLayersV2(\n",
       "                (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=same, padding_mode=circular)\n",
       "                (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=same, padding_mode=circular)\n",
       "                (relu): ReLU()\n",
       "              )\n",
       "            )\n",
       "            (_antialias_layer): LowPassFilter(in_channels=512, filter_size=3, padding=same, padding_mode=circular)\n",
       "          )\n",
       "          (1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): PolyphaseInvariantDown2D(\n",
       "            (component_selection): LPS()\n",
       "            (_antialias_layer): LowPassFilter(in_channels=1024, filter_size=3, padding=same, padding_mode=circular)\n",
       "          )\n",
       "          (1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BottleneckLiteCustom(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): BottleneckLiteCustom(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=1024, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74e0231",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8f411f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
